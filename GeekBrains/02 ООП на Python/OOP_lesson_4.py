# ООП на Python (урок № 4).
# Упражнение продолжает практическую работу из последнего видеоурока. Для усовершенствования
# приложения разберитесь, как можно реализовать получение common words с соседних страниц — тех, на которые есть ссылки.
# Возможен следующий алгоритм решения задачи:
# Получить ссылки на соседние страницы. Для этого можно воспользоваться библиотекой BeautifulSoup. Не забудьте отобрать
# только правильные ссылки, которые указывают на другие страницы Википедии. (Вы можете распознать их по тексту \wiki).
# Спарсить отдельно соседние страницы. Для этого вам необходимо перебрать в цикле все полученные ссылки.
# Сложить все в один список.

import requests, json, re
from urllib.request import unquote, quote
from bs4 import BeautifulSoup as BS

# Функция, возвращающая список кортежей из слов и количества их упоминаний в контентном блоке wiki-странички,
# отсортированный по уменьшению кол-ва вхождения слов

def get_common_words(topic):
    bs_obj = BS(requests.get(f'https://ru.wikipedia.org/wiki/{topic}').text, 'html.parser')
# Из всей страницы работаем только с тегом, в блоке которого находиться весь интересующий контент. Полученный текст
# приводим к нижнему регистру для сравнимости слов далее
    bs_obj_text = bs_obj.find(id="mw-content-text").div.text.lower()
# Отдбираем все русские слова длиннее трёх букв, формируем список
    words_list = re.findall('[а-яА-Я]{4,}', bs_obj_text)
    words_set = set(words_list)
    words_count = {}
    for word in words_set:
        words_count[word] = words_list.count(word)
    return sorted(list(words_count.items()), key=lambda item: -item[1])

# Функция, возвращающая первые 200 "правильных" ссылок из блока контента обрабатываемой wiki-странички

def parsing_first_200_links(topic):
    bs_obj = BS(requests.get(f'https://ru.wikipedia.org/wiki/{topic}').text, 'html.parser')
    bs_obj = bs_obj.find(id="mw-content-text").div
    all_links_list = bs_obj.find_all('a')
    good_links_list = []
    for link in all_links_list:
# Исключаю из списка ссылок слова, которые мне не понравились...имею право
        try:
            if  (link['href'].startswith('/wiki/') and
                re.findall('[а-яА-Я_()0-9\-]', unquote(link['href']).replace('/wiki/', '')) != [] and
                re.findall('[a-zA-Z]', unquote(link['href']).replace('/wiki/', '')) == []):
                if (link['href'].find(':') < 1 and link['href'].find(quote('Файл')) < 1 and
                                                   link['href'].find(quote('язык')) < 1 and
                                                   link['href'].find(quote('библиот')) < 1 and
                                                   link['href'].find(quote('Библиот')) < 1):
                    good_links_list.append(unquote(link['href'].replace('/wiki/', '')))
        except KeyError:
            pass
    return good_links_list[:200] if len(good_links_list) > 200 else good_links_list

# Функция объединения двух списков кортежей с сортировкой результирующего

def concatinated_lists(lst1, lst2):
    lst1 = dict(lst1[:])
    lst2 = dict(lst2[:])
    for key2 in lst2:
        check = False
        for key1 in lst1:
            if key1 == key2:
                lst1[key1] += lst2[key2]
                check = True
        if not check:
            lst1[key2] = lst2[key2]
    return sorted(list(lst1.items()), key=lambda item: -item[1])

# Парсинг wiki-страничек по ссылкам с интересующей (topic) странички, по умолчанию парсится первых 10 ссылок

def parsing_neighbor_pages(topic, count=10):
    links_list = parsing_first_200_links(topic)
    result_commom_wodrs = []
    for link in links_list[:count]:
        result_commom_wodrs = concatinated_lists(result_commom_wodrs, get_common_words(link))
    return result_commom_wodrs


if __name__ == '__main__':
    print(get_common_words('Россия')[:50])
# Вывод вида - "[('россии', 430), ('года', 238), ('российской', 154), ('году', 147), ('россия', 136), ('федерации', 119),
#                ('ссср', 105), ('также', 82), ('российская', 78), ('населения', 75), ('неопр', 75), ('мире', 65),
#                ('страны', 60), ('мира', 54), ('статья', 54), ('обращения', 53), ('дата', 53), ('место', 52),
#                ('государственной', 50), ('является', 50), ('около', 48), ('стран', 47), ('основная', 47),
#                ('архивировано', 47), ('государства', 47), ('республика', 46), ('века', 44), ('более', 39),
#                ('территории', 38), ('млрд', 38), ('диссертации', 37), ('автореферат', 37), ('государство', 37),
#                ('было', 37), ('история', 36), ('были', 36), ('время', 36), ('наиболее', 33), ('после', 33),
#                ('человек', 32), ('составляет', 32), ('июля', 31), ('часть', 30), ('ссылка', 30), ('недоступная', 30),
#                ('образования', 30), ('русский', 30), ('согласно', 30), ('федерация', 29), ('москва', 28)]

    print(parsing_neighbor_pages('Россия')[:50])
# Вывод вида - "[('года', 861), ('россии', 569), ('российской', 448), ('крыма', 372), ('федерации', 330), ('править', 330),
#                ('крым', 278), ('году', 265), ('украины', 239), ('рсфср', 215), ('флаг', 206), ('россия', 204),
#                ('дата', 195), ('ссср', 176), ('марта', 175), ('также', 162), ('было', 160), ('после', 148),
#                ('обращения', 144), ('крыму', 140), ('флага', 140), ('российская', 140), ('герб', 130),
#                ('республика', 130), ('была', 121), ('неопр', 119), ('история', 119), ('герба', 116), ('декабря', 116),
#                ('были', 114), ('совета', 111), ('территории', 106), ('республики', 104), ('государственного', 103),
#                ('февраля', 98), ('время', 98), ('власти', 95), ('государства', 95), ('гимна', 93), ('века', 90),
#                ('президента', 89), ('состав', 88), ('империи', 88), ('однако', 85), ('государственный', 82),
#                ('конституции', 82), ('апреля', 81), ('монеты', 80), ('область', 80), ('федерация', 80)]